{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664a2934-accd-4538-b47b-a3432b214e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Deeksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c5ef01-3b74-4eda-b87d-4fbd774a4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "def fetch_page(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Fetch a page and return parsed HTML.\"\"\"\n",
    "    print(f\"ğŸ“¥ Fetching: {url}\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Educational Scraper)'}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        return BeautifulSoup(resp.text, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_quotes_from_page(soup: BeautifulSoup) -> list:\n",
    "    \"\"\"Extract quotes and authors from quotes.toscrape.com.\"\"\"\n",
    "    quotes = []\n",
    "    for quote_div in soup.select('.quote'):\n",
    "        text = quote_div.select_one('.text').get_text(strip=True)\n",
    "        author = quote_div.select_one('.author').get_text(strip=True)\n",
    "        tags = [t.get_text(strip=True) for t in quote_div.select('.tag')]\n",
    "        quotes.append({\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        })\n",
    "    return quotes\n",
    "\n",
    "def scrape_quotes_toscrape(num_pages: int = 3) -> list:\n",
    "    \"\"\"Scrape multiple pages from quotes.toscrape.com.\"\"\"\n",
    "    all_quotes = []\n",
    "    base_url = \"https://quotes.toscrape.com/page/{}/\"\n",
    "    \n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = base_url.format(page)\n",
    "        soup = fetch_page(url)\n",
    "        if soup:\n",
    "            quotes = extract_quotes_from_page(soup)\n",
    "            all_quotes.extend(quotes)\n",
    "            print(f\"  â• Got {len(quotes)} quotes (total: {len(all_quotes)})\")\n",
    "        \n",
    "        # Be polite: pause between requests\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    return all_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3520d8-d263-4515-9f61-00ac8addbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaner.py\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download once (or ensure it's in setup)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text for NLP: lowercase, remove punctuation, strip stopwords.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_quotes(quotes: list) -> list:\n",
    "    \"\"\"Apply cleaning to a list of quote dicts.\"\"\"\n",
    "    print(\"ğŸª¥ Cleaning text...\")\n",
    "    cleaned = []\n",
    "    for q in quotes:\n",
    "        cleaned_text = clean_text(q['text'])\n",
    "        if cleaned_text:  # skip empty\n",
    "            cleaned.append({\n",
    "                'original_text': q['text'],\n",
    "                'cleaned_text': cleaned_text,\n",
    "                'author': q['author'],\n",
    "                'tags': q['tags']\n",
    "            })\n",
    "    print(f\"âœ… Cleaned {len(cleaned)} quotes\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e1791e-841d-472b-98ff-856a7ff71c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install if needed (run once)\n",
    "!pip install requests beautifulsoup4 nltk\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927d0157-867d-4202-8b7b-b771215fbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords (safe to run multiple times)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8e4ed8-e9f0-47be-ba71-5912b1932502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str) -> BeautifulSoup:\n",
    "    print(f\"ğŸ“¥ Fetching: {url}\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Educational Scraper)'}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        return BeautifulSoup(resp.text, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_quotes_from_page(soup: BeautifulSoup) -> list:\n",
    "    quotes = []\n",
    "    for quote_div in soup.select('.quote'):\n",
    "        text = quote_div.select_one('.text').get_text(strip=True)\n",
    "        author = quote_div.select_one('.author').get_text(strip=True)\n",
    "        tags = [t.get_text(strip=True) for t in quote_div.select('.tag')]\n",
    "        quotes.append({'text': text, 'author': author, 'tags': tags})\n",
    "    return quotes\n",
    "\n",
    "def scrape_quotes_toscrape(num_pages: int = 2) -> list:\n",
    "    all_quotes = []\n",
    "    base_url = \"https://quotes.toscrape.com/page/{}/\"\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = base_url.format(page)\n",
    "        soup = fetch_page(url)\n",
    "        if soup:\n",
    "            quotes = extract_quotes_from_page(soup)\n",
    "            all_quotes.extend(quotes)\n",
    "            print(f\"  â• Got {len(quotes)} quotes\")\n",
    "        time.sleep(1)  # be polite\n",
    "    return all_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73283139-689b-40c2-ae0a-492b9796b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # keep only letters/spaces\n",
    "    words = [w for w in text.split() if w not in STOP_WORDS and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_quotes(quotes: list) -> list:\n",
    "    print(\"ğŸª¥ Cleaning text...\")\n",
    "    cleaned = []\n",
    "    for q in quotes:\n",
    "        cleaned_text = clean_text(q['text'])\n",
    "        if cleaned_text:\n",
    "            cleaned.append({\n",
    "                'original_text': q['text'],\n",
    "                'cleaned_text': cleaned_text,\n",
    "                'author': q['author'],\n",
    "                'tags': q['tags']\n",
    "            })\n",
    "    print(f\"âœ… Cleaned {len(cleaned)} quotes\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f115b12-c049-49db-95b4-405c90ead02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Project 2 ETL...\n",
      "ğŸ“¥ Fetching: https://quotes.toscrape.com/page/1/\n",
      "  â• Got 10 quotes\n",
      "ğŸ“¥ Fetching: https://quotes.toscrape.com/page/2/\n",
      "  â• Got 10 quotes\n",
      "ğŸª¥ Cleaning text...\n",
      "âœ… Cleaned 20 quotes\n",
      "âœ… Saved to output/cleaned_quotes.json\n",
      "\n",
      "ğŸ‰ Done! Check the 'output' folder.\n"
     ]
    }
   ],
   "source": [
    "def save_to_json(data: list, filepath: str):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Saved to {filepath}\")\n",
    "\n",
    "# Run full pipeline\n",
    "print(\"ğŸš€ Starting Project 2 ETL...\")\n",
    "raw_data = scrape_quotes_toscrape(num_pages=2)\n",
    "\n",
    "if raw_data:\n",
    "    cleaned_data = clean_quotes(raw_data)\n",
    "    save_to_json(cleaned_data, \"output/cleaned_quotes.json\")\n",
    "    print(\"\\nğŸ‰ Done! Check the 'output' folder.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data scraped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae7fb63-9c5c-4490-b55b-3bcdd2a74821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Quote 1 ---\n",
      "Original: â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€\n",
      "Cleaned:  world created process thinking cannot changed without changing thinking\n",
      "Author:   Albert Einstein\n",
      "\n",
      "--- Quote 2 ---\n",
      "Original: â€œIt is our choices, Harry, that show what we truly are, far more than our abilities.â€\n",
      "Cleaned:  choices harry show truly far abilities\n",
      "Author:   J.K. Rowling\n"
     ]
    }
   ],
   "source": [
    "# Load and display first 2 cleaned quotes\n",
    "with open(\"output/cleaned_quotes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i, quote in enumerate(data[:2]):\n",
    "    print(f\"\\n--- Quote {i+1} ---\")\n",
    "    print(\"Original:\", quote[\"original_text\"])\n",
    "    print(\"Cleaned: \", quote[\"cleaned_text\"])\n",
    "    print(\"Author:  \", quote[\"author\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e0480-6e85-4213-bd0c-01556f0acfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
